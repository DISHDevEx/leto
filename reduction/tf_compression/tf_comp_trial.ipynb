{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/github/leto/reduction/tf_compression\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 16:27:43.690306: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-15 16:27:43.741645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 16:27:44.506482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Converts an image between PNG and TFCI formats.\n",
    "\n",
    "Use this script to compress images with pre-trained models as published. See the\n",
    "'models' subcommand for a list of available models.\n",
    "\n",
    "This script requires TFC v2 (`pip install tensorflow-compression==2.*`).\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "from absl import app\n",
    "from absl.flags import argparse_flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_compression as tfc  # pylint:disable=unused-import\n",
    "\n",
    "\n",
    "# Default URL to fetch metagraphs from.\n",
    "URL_PREFIX = \"https://storage.googleapis.com/tensorflow_compression/metagraphs\"\n",
    "# Default location to store cached metagraphs.\n",
    "METAGRAPH_CACHE = \"/tmp/tfc_metagraphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models():\n",
    "  \"\"\"Lists available models in web storage with a description.\"\"\"\n",
    "  url = URL_PREFIX + \"/models.txt\"\n",
    "  request = urllib.request.urlopen(url)\n",
    "  try:\n",
    "    print(request.read().decode(\"utf-8\"))\n",
    "  finally:\n",
    "    request.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following models are available:\n",
      "\n",
      "* Models published in:\n",
      "  F. Mentzer, G. Toderici, M. Tschannen, E. Agustsson:\n",
      "  \"High-Fidelity Generative Image Compression\"\n",
      "  Adv. in Neural Information Processing Systems 33 (NeurIPS 2020)\n",
      "\n",
      "  hific-lo\n",
      "  hific-mi\n",
      "  hific-hi\n",
      "\n",
      "  These are the GAN-based models that were used for the paper and the demo\n",
      "  website (https://hific.github.io/). The target bitrates of these models are\n",
      "  0.14 bpp for \"lo\", 0.3 bpp for \"mi\", and 0.45 bpp for \"hi\".\n",
      "\n",
      "* Models published in:\n",
      "  D. Minnen, S. Singh:\n",
      "  \"Channel-wise Autoregressive Entropy Models for Learned Image Compression\"\n",
      "  Int. Conf. on Image Processing (ICIP 2020)\n",
      "\n",
      "  ms2020-cc10-mse-[1-10]\n",
      "  ms2020-cc8-msssim-[1-9]\n",
      "\n",
      "  These are channel-wise autoregressive models optimized for either MSE or\n",
      "  MS-SSIM. The MSE models have 10 splits along the channel dimension so a\n",
      "  latent tensor with 320 channels is split into 10 blocks with 32 channels\n",
      "  each. The MS-SSIM models have eight splits with 40 channels each.\n",
      "\n",
      "  All of the models include latent residual prediction (LRP) and were optimized\n",
      "  for 5,000,000 steps (MSE) or 4,000,000 steps (MS-SSIM) using Adam. The number\n",
      "  at the end of the name (1-10 or 1-9) indicates the quality level where larger\n",
      "  numbers imply higher quality and higher bit rates.\n",
      "\n",
      "  Although not used in the paper, we've been referring to this model as\n",
      "  \"CHARM\" for \"CHannel-wise AutoRegressive Model\".\n",
      "\n",
      "  Note: this model does not use the integer modification described below, so it\n",
      "  may be necessary to disable the GPU in order to reliably encode and decode.\n",
      "  This can be done by setting the environment variable `CUDA_VISIBLE_DEVICES`\n",
      "  to the empty string, e.g., using the command line:\n",
      "  CUDA_VISIBLE_DEVICES='' python tfci.py ...\n",
      "\n",
      "* Models published in:\n",
      "  D. Minnen, J. Ballé, G.D. Toderici:\n",
      "  \"Joint Autoregressive and Hierarchical Priors for Learned Image Compression\"\n",
      "  Adv. in Neural Information Processing Systems 31 (NeurIPS 2018)\n",
      "\n",
      "  mbt2018-mean-mse-[1-8]\n",
      "  mbt2018-mean-msssim-[1-8]\n",
      "\n",
      "  These are hyperprior models with non zero-mean Gaussian conditionals (without\n",
      "  autoregression), optimized for MSE (mean squared error) and MS-SSIM\n",
      "  (multiscale SSIM), respectively. The number 1-8 at the end indicates the\n",
      "  quality level (1: lowest, 8: highest).\n",
      "\n",
      "  While generalizing the hyperprior models to non-zero mean distributions\n",
      "  translates into some compression gains, the gains are not as high as for a\n",
      "  combined autoregressive and hierarchical prior. However, the runtime for\n",
      "  autoregressive priors is generally quite high.\n",
      "\n",
      "* Models published in:\n",
      "  J. Ballé, D. Minnen, S. Singh, S.J. Hwang, N. Johnston:\n",
      "  \"Variational Image Compression with a Scale Hyperprior\"\n",
      "  Int. Conf. on Learning Representations (ICLR), 2018\n",
      "\n",
      "  bmshj2018-factorized-mse-[1-8]\n",
      "  bmshj2018-factorized-msssim-[1-8]\n",
      "  bmshj2018-hyperprior-mse-[1-8]\n",
      "  bmshj2018-hyperprior-msssim-[1-8]\n",
      "\n",
      "  These are the factorized prior and hyperprior models optimized for MSE (mean\n",
      "  squared error) and MS-SSIM (multiscale SSIM), respectively. The number 1-8 at\n",
      "  the end indicates the quality level (1: lowest, 8: highest).\n",
      "\n",
      "  These models demonstrate the bit rate savings achieved by a hierarchical vs.\n",
      "  a factorized prior (entropy model).\n",
      "\n",
      "* Models published in:\n",
      "  J. Ballé:\n",
      "  \"Efficient Nonlinear Transforms for Lossy Image Compression\"\n",
      "  Picture Coding Symposium (PCS), 2018\n",
      "\n",
      "  b2018-leaky_relu-128-[1-4]\n",
      "  b2018-leaky_relu-192-[1-4]\n",
      "  b2018-gdn-128-[1-4]\n",
      "  b2018-gdn-192-[1-4]\n",
      "\n",
      "  These are nonlinear transform coders with factorized priors (entropy models)\n",
      "  optimized for MSE (mean squared error), with either leaky ReLU and GDN\n",
      "  activation functions, and 128 or 192 filters per layer. The number 1-4 at\n",
      "  the end indicates the quality level (1: lowest, 4: highest).\n",
      "\n",
      "  These models demonstrate the higher representational efficiency of GDN vs.\n",
      "  scalar activation functions such as leaky ReLU.\n",
      "\n",
      "Note: all the models using hierarchical/hyperpriors published above include a\n",
      "modification from:\n",
      "J. Ballé, N. Johnston, D. Minnen:\n",
      "\"Integer Networks for Data Compression with Latent-Variable Models\"\n",
      "Int. Conf. on Learning Representations (ICLR), 2019\n",
      "\n",
      "The conditional priors are modified to use integer arithmetic, in order to\n",
      "ensure consistent results across different hardware platforms. Other than this,\n",
      "the models are reproductions of the original models used in the publication.\n",
      "Therefore, performance may differ slightly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-compression~=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: scipy~=1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow-compression~=2.13.0) (1.11.0)\n",
      "Requirement already satisfied: tensorflow~=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow-compression~=2.13.0) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-probability~=0.15 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow-compression~=2.13.0) (0.21.0)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scipy~=1.4->tensorflow-compression~=2.13.0) (1.24.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.56.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.33.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow-probability~=0.15->tensorflow-compression~=2.13.0) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow-probability~=0.15->tensorflow-compression~=2.13.0) (2.2.1)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow-probability~=0.15->tensorflow-compression~=2.13.0) (0.1.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.3.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.26.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow-compression~=2.13.0) (3.2.2)\n",
      "git: 'clonehttps://github.com/tensorflow/compression.git' is not a git command. See 'git --help'.\n"
     ]
    }
   ],
   "source": [
    "# Installs the latest version of TFC compatible with the installed TF version.\n",
    "!pip install tensorflow-compression~=$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\\d+\\.\\d+).*/\\1.0/sg')\n",
    "\n",
    "# Downloads the 'models' directory from Github.\n",
    "![[ -e /tfc ]] || git clonehttps://github.com/tensorflow/compression.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/github/leto/reduction/tf_compression\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38822/3772994803.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tfci'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Javascript\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display, HTML\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtfci\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tf\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39msetLevel(\u001b[39m'\u001b[39m\u001b[39mWARN\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Only show Warnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tfci'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "from PIL import Image\n",
    "from IPython.display import Image as DisplayImage\n",
    "from IPython.display import Javascript\n",
    "from IPython.core.display import display, HTML\n",
    "import tfci\n",
    "import urllib.request\n",
    "\n",
    "tf.get_logger().setLevel('WARN')  # Only show Warnings\n",
    "\n",
    "FILES_DIR = '/content/files'\n",
    "OUT_DIR = '/content/out'\n",
    "DEFAULT_IMAGE_URL = ('https://storage.googleapis.com/hific/clic2020/'\n",
    "                     'images/originals/ad249bba099568403dc6b97bc37f8d74.png')\n",
    "\n",
    "os.makedirs(FILES_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "File = collections.namedtuple('File', ['full_path', 'num_bytes', 'bpp'])\n",
    "\n",
    "def print_html(html):\n",
    "  display(HTML(html + '<br/>'))\n",
    "\n",
    "def get_default_image(output_dir):\n",
    "  output_path = os.path.join(output_dir, os.path.basename(DEFAULT_IMAGE_URL))\n",
    "  print('Downloading', DEFAULT_IMAGE_URL, '\\n->', output_path)\n",
    "  urllib.request.urlretrieve(DEFAULT_IMAGE_URL, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
